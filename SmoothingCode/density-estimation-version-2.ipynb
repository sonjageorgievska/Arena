{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and rebuild data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the fitted positions data and build a Pandas DataFrame called 'frame'.\n",
    "In 'frame' each row is a fitted position.\n",
    "The columns contain the separate pieces of information accompanying each fit, such as the coordinates, \n",
    "timestamp, and uncertainty values.\n",
    "The rows are ordered by timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# real dataset\n",
    "\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open(\"/Users/philip/Documents/PhD/data/ArenaData/arena_fits/2015-07-05.json\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "json_lines = []\n",
    "\n",
    "for line in data:\n",
    "    jsline = json.loads(line)\n",
    "    json_lines.append(jsline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "frame = pd.DataFrame.from_dict(json_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rebuild dataframe\n",
    "# make dataframe of dicts nested in 'value' column\n",
    "value = pd.DataFrame(list(frame['value']))\n",
    "del frame['value']\n",
    "\n",
    "# make dataframe of dicts nested in 'trackeeHistory' column\n",
    "trackee = pd.DataFrame(list(value['trackeeHistory']))\n",
    "del value['trackeeHistory']\n",
    "\n",
    "chi2PerDof = pd.DataFrame(list(trackee['chi2PerDof']))\n",
    "chi2PerDof.columns = ['chi2PerDof']\n",
    "probChi2 = pd.DataFrame(list(trackee['probChi2']))\n",
    "probChi2.columns = ['probChi2']\n",
    "nMeasurements = pd.DataFrame(list(trackee['nMeasurements']))\n",
    "nMeasurements.columns = ['nMeasurements']\n",
    "localMac = pd.DataFrame(list(trackee['localMac']))\n",
    "localMac.columns = ['localMac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataframe with a 'coordinates' column\n",
    "averagecoordinate = pd.DataFrame(list(value['averagecoordinate']))\n",
    "coordinates = pd.DataFrame(list(averagecoordinate['avg']))\n",
    "averagecoordinate = averagecoordinate.join(coordinates)\n",
    "error = pd.DataFrame(list(averagecoordinate['error']))\n",
    "errorcoordinates = pd.DataFrame(list(error['coordinates']))\n",
    "del errorcoordinates[2]\n",
    "errorcoordinates.columns = ['x_error','y_error']\n",
    "\n",
    "del averagecoordinate['avg']\n",
    "del value['averagecoordinate']\n",
    "\n",
    "# join dataframes\n",
    "frame = frame.join(value.join(averagecoordinate))\n",
    "frame = frame.join(chi2PerDof)\n",
    "frame = frame.join(probChi2)\n",
    "frame = frame.join(errorcoordinates)\n",
    "frame = frame.join(localMac)\n",
    "frame = frame.join(nMeasurements)\n",
    "del frame['regionsNodesIds']\n",
    "del frame['error']\n",
    "del frame['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = frame.sort_values(by='measurementTimestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = frame[frame['localMac'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start data analysis code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density estimation code works according to the following steps:\n",
    "- First a time window is selected from the data\n",
    "- The set of unique MAC addresses in the time window is determined\n",
    "- A bunch of dictionairies are created which hold for each MAC (as key), the required values to construct\n",
    "the density estimate, such as the fitted position coordinates, and the associated uncertainty values\n",
    "- After the first time window, the dictionairies are recreated from the previous set, so the set of MAC addresses\n",
    "can only expand\n",
    "- The density estimates are calculated\n",
    "- The density estimates are summed, to create the total crowd density estimate\n",
    "- After each iteration, a deep copy is made of all the density estimates \n",
    "- If a MAC address is not detected in a new time window, the previous density estimate is smoothed (stored in the deep copy)\n",
    "- The smoothing is only done if the history value associated with the MAC address does not exceed the memory parameter\n",
    "- If the history value does exceed the memory parameter, the density estimate remains zero until the MAC is detected again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here it is assumed that the data is stored in a Pandas DataFrame called 'frame'.\n",
    "The function 'selectWindow' selects the part of the dataset with timestamps falling within the interval specified by variables 'start' and 'stop', and returns a DataFrame with the same structure as 'frame'.\n",
    "Start and stop are specified by the iterator k and the parameters 'interval' and 'timestep'.\n",
    "If timestep > interval, the time windows are non-overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectWindow(k):\n",
    "    start = min(frame['measurementTimestamp']) + k * timestep\n",
    "    stop = start + interval\n",
    "\n",
    "    window = frame[(frame['measurementTimestamp'] >= start) & \n",
    "                       (frame['measurementTimestamp'] < stop)]\n",
    "\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function createDataStructures returns a bunch of dictionairies for the MAC addresses detected in the \n",
    "selected time window, required to do the density estimation later on.\n",
    "It creates a Python dictionairy called 'histos' with the MAC addresses as keys. Each address gets an empty grid (zeros), \n",
    "which is the two-dimensional probability distribution yet to be evaluated.\n",
    "It creates a second dictionairy called 'positions' where each MAC address gets an empty list.\n",
    "It creates two separate dictionairies for the uncertainty values in x and y direction,\n",
    "where each MAC address gets an empty list.\n",
    "It creates a dictionairy called 'history', which for each MAC keeps track of the time that has passed since the last update, given by the number of time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataStructures(window):\n",
    "    grids = np.zeros((len(set(window['sourceMac'])), height,width))\n",
    "\n",
    "    # dictionary of histograms (with mac addresses as keys)\n",
    "    histos = dict(zip(set(window['sourceMac']), grids))\n",
    "    \n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    positions = dict(zip(set(window['sourceMac']), [[] for i in range(len(set(window['sourceMac'])))]))\n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    x_errors = dict(zip(set(window['sourceMac']), emptylist))\n",
    "    emptylist = [[] for i in range(len(set(window['sourceMac'])))]\n",
    "    y_errors = dict(zip(set(window['sourceMac']), emptylist))\n",
    "    \n",
    "    history = dict(zip(set(window['sourceMac']), np.zeros(len(set(window['sourceMac'])))))\n",
    "    \n",
    "    return histos, positions, x_errors, y_errors, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function resetDataStructures all the dictionairies created in createDataStructures are reset: all the MAC addresses get an empty list again.\n",
    "The dictionairy containing the calculated density estimates is deep copied, for possible smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetDataStructures(histos):\n",
    "    \n",
    "    histos_old = copy.deepcopy(histos)\n",
    "    \n",
    "    grids = np.zeros((len(histos), height,width))\n",
    "    histos = dict(zip(histos.keys(), grids))\n",
    "    \n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    positions = dict(zip(histos.keys(), emptylist))\n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    x_errors = dict(zip(histos.keys(), emptylist))\n",
    "    emptylist = [[] for i in range(len(histos))]\n",
    "    y_errors = dict(zip(histos.keys(), emptylist))\n",
    "    \n",
    "    return histos, histos_old, positions, x_errors, y_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function updateDataStructures all the empty lists in the dictionairies created in createDataStructures are filled with values from the data in the selected time window.\n",
    "If a MAC address is not yet in the dictionary, it is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateDataStructures(window, histos, positions, x_errors, y_errors, history):\n",
    "    for i in range(len(window)):\n",
    "        if not window['sourceMac'].values[i] in positions:\n",
    "            histos[window['sourceMac'].values[i]] = np.zeros((height,width))\n",
    "            positions[window['sourceMac'].values[i]] = []\n",
    "            x_errors[window['sourceMac'].values[i]] = []\n",
    "            y_errors[window['sourceMac'].values[i]] = []\n",
    "            history[window['sourceMac'].values[i]] = 0\n",
    "            \n",
    "        positions[window['sourceMac'].values[i]].append(window['coordinates'].values[i])\n",
    "        x_errors[window['sourceMac'].values[i]].append(window['x_error'].values[i])\n",
    "        y_errors[window['sourceMac'].values[i]].append(window['y_error'].values[i])\n",
    "        \n",
    "    return histos, positions, x_errors, y_errors, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function createDensityEstimates all the actual magic happens.\n",
    "We iterate over the MAC addresses, for each MAC address we iterate over the evaluation grid.\n",
    "At each evaluation grid point, we iterate over the number of fitted positions (data points) found in the time window for that MAC address.\n",
    "For each data point, we collect the x and y coordinates and the x and y uncertainty values,\n",
    "and calculate the contribution we get from that data point by evaluating the kernel function.\n",
    "The origin (0,0) of the grid is centered on the center of the football field.\n",
    "The kernel function takes as arguments the x and y coordinate of the distance between the evaluation\n",
    "grid point and the data point in meters.\n",
    "If the errors are zero, we check whether the data point is in the same cell as the grid point we are evaluating.\n",
    "If that is the case, the data point contributes 1 (unity) to our evaluation grid point.\n",
    "We multiply (scale) by the cell area of our evalution grid to get probabilities per cell area (and not per square meter).\n",
    "Finally, we normalize our density estimate by the value we get from integrating over the evaluation grid field (and omit normalizing by the number N of data points),\n",
    "because we assume that the probability is unity that the mobile device is somewhere inside the Arena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDensityEstimates(window, histos, positions, x_errors, y_errors):\n",
    "\n",
    "    for mac in histos.keys():\n",
    "        if len(positions[mac]) > 0:          \n",
    "            for u in range(width):\n",
    "                for v in range(height):\n",
    "                    for j in range(len(positions[mac])):\n",
    "                        xi = positions[mac][j][0]\n",
    "                        yi = positions[mac][j][1]\n",
    "                        \n",
    "                        x = u * cellsize - 120\n",
    "                        y = v * cellsize - 90\n",
    "\n",
    "                        sigma_x = x_errors[mac][j]\n",
    "                        sigma_y = y_errors[mac][j]\n",
    "\n",
    "                        if sigma_x > 0 and sigma_y > 0:\n",
    "                            histos[mac][v][u] += cellsize**2 *\\\n",
    "                            kernel((x - xi), sigma_x) * kernel((y - yi), sigma_y)\n",
    "                        else:\n",
    "                            if abs((x - xi)) < cellsize/2. and abs((y - yi)) < cellsize/2.:\n",
    "                                 histos[mac][v][u] = 1\n",
    "                        \n",
    "                    #histos[mac][v][u] /= len(positions[mac])\n",
    "            if histos[mac].sum() > 0:\n",
    "                histos[mac] /= histos[mac].sum()\n",
    "    return histos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel is the Gaussian kernel function, without normalization term, because the density estimates are normalized\n",
    "by the value resulting from integrating over the evaluation grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel(x, sigma): \n",
    "    return exp(-(x**2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function smoothNonUpdatedEstimates previously calculated density estimates are smoothed.\n",
    "We first construct a two-dimensional Gaussian bump  of which the width (sigma) is based on by pedestrian walking speed.\n",
    "The two-dimensional function is created using a scipy.stats library function.\n",
    "If there were no detections for a MAC address in the time window, its density estimate from the previous time window \n",
    "(stored in a deep copy) is convoluted with the Gaussian bivariate bump, using a library function from the scipy signal processing module.\n",
    "Each time this is done, the history value associated with the MAC address is incremented.\n",
    "If the history value exceeds the memory parameter value, the density estimate remains zero.\n",
    "The density estimate remains zero, untill the MAC is detected again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smoothNonUpdatedEstimates(histos, histos_old, positions, history):\n",
    "    \n",
    "    # generate weighting function with dispersion set to \n",
    "    # Brownian motion with v = 0.5 m/s and t = interval time\n",
    "    # diffusion constant D = (v^2)/2\n",
    "    D = 0.5 # this assumes a walking speed of 0.71 m/s \n",
    "    t = interval / 1000\n",
    "    sigma = sqrt(2*D*t) / cellsize\n",
    "    \n",
    "    var = multivariate_normal(mean=[width/2 - 1,height/2 - 1], cov=[[sigma**2,0],[0,sigma**2]])\n",
    "    \n",
    "    weights = np.zeros((height,width))\n",
    "    for i in np.arange(width):\n",
    "        for j in np.arange(height):\n",
    "            weights[j][i] += var.pdf([i,j])\n",
    "    \n",
    "    for mac in histos.keys():\n",
    "        if len(positions[mac]) == 0:\n",
    "            if history[mac] < memory:\n",
    "                # smooth existing pdf from previous time interval\n",
    "                # apply a convolution\n",
    "\n",
    "                conv = signal.convolve2d(histos_old[mac], weights, boundary='wrap', mode='same')\n",
    "                \n",
    "                histos[mac] += conv\n",
    "                history[mac] += 1\n",
    "            else:\n",
    "                history[mac] = 0\n",
    "    \n",
    "    return histos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sumHistograms function simply sums the histograms in the dictionairy 'histos', \n",
    "and returns the total density estimate in the form of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sumHistograms(histos):\n",
    "    # total density histogram per period\n",
    "    total_dens_histo = np.zeros((height, width))\n",
    "    \n",
    "    for mac in histos.keys():\n",
    "        total_dens_histo += histos[mac]\n",
    "                \n",
    "    return total_dens_histo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the __main__ function. It runs all the steps, and writes the total density estimate to file.\n",
    "It differentiates between the first and later iterations, in order to initialize the dictionairies,\n",
    "and then only to reset them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runDataAnalysis():\n",
    "    \n",
    "    for k in range(periods):\n",
    "        window = selectWindow(k)\n",
    "        \n",
    "        if k < 1:\n",
    "            histos, positions, x_errors, y_errors, history = createDataStructures(window)\n",
    "            histos, positions, x_errors, y_errors, history =\\\n",
    "            updateDataStructures(window, histos, positions, x_errors, y_errors, history)\n",
    "            histos = createDensityEstimates(window, histos, positions, x_errors, y_errors)\n",
    "        else:\n",
    "            histos, histos_old, positions, x_errors, y_errors = resetDataStructures(histos)\n",
    "            histos, positions, x_errors, y_errors, history =\\\n",
    "            updateDataStructures(window, histos, positions, x_errors, y_errors, history)\n",
    "            histos = createDensityEstimates(window, histos, positions, x_errors, y_errors)\n",
    "            histos = smoothNonUpdatedEstimates(histos, histos_old, positions, history)\n",
    "        \n",
    "        total_dens_histo = sumHistograms(histos)\n",
    "        \n",
    "        #print(len(histos), total_dens_histo.sum())\n",
    "        \n",
    "        np.savetxt('output/dens_histo_%d.csv' %  k, total_dens_histo, delimiter=',')\n",
    "        print('Time window:', k)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell the parameters are set to run runDataAnalysis. The variable 'cellsize' sets the distance between\n",
    "grid points in the evaluation grid.\n",
    "The variables height and width follow from dividing the size of the evaluation grid (240x180 meter), \n",
    "which is the rectangle containing the Arena stadium, by the cellsize.\n",
    "The variable 'periods' sets the number of time windows to run the analysis.\n",
    "Interval is the length of the time window (in milliseconds). Timestep is the amount of time the time window is moved at each iteration.\n",
    "The memory variable determines the amount of time, measured by the number of time windows,\n",
    "a density estimate is held in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt, pi, exp\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import signal\n",
    "import copy\n",
    "\n",
    "# cell size (bin size)\n",
    "cellsize = 3;\n",
    "# size of binned region (number of cells)\n",
    "width = int(240/cellsize); height = int(180/cellsize)\n",
    "\n",
    "# numbers of time intervals\n",
    "periods = 60\n",
    "timestep = 30000 # 10000\n",
    "interval = 120000 # 30000\n",
    "memory = 0\n",
    "\n",
    "runDataAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check maximum value for z-axis limit\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "maxValue = 0\n",
    "\n",
    "for i in range(periods):\n",
    "    temp = np.loadtxt('output/brownian-smoothing-test_%d.csv' % i, delimiter=',').max()\n",
    "    if temp > maxValue:\n",
    "        maxValue = temp\n",
    "        \n",
    "#maxValue = ceil(maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "\n",
    "#col = ['r', 'y', 'c', 'k', 'c','r'] * height * width\n",
    "col = ['w','r','w','w','w','w'] * height * width\n",
    "# colors = np.random.choice(col, height*width)\n",
    "\n",
    "for k in range(periods):\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x_data, y_data = np.meshgrid( np.arange(width),\n",
    "                                  np.arange(height)*(-1) )\n",
    "\n",
    "    x_data = x_data.flatten()\n",
    "    y_data = y_data.flatten()\n",
    "\n",
    "    z_data = np.loadtxt('output/brownian-smoothing-test_%s.csv' % k, delimiter=',').flatten()\n",
    "    #z_data = total_dens_histos[k].flatten()\n",
    "    ax.set_zlim3d(0, maxValue)\n",
    "    ax.bar3d( x_data,\n",
    "              y_data,\n",
    "              np.zeros(len(z_data)),\n",
    "              1, 1, z_data, color=col) # \n",
    "    if k < 10:\n",
    "        number = '000' + str(k)\n",
    "    elif k > 9:\n",
    "        number = '00' + str(k)\n",
    "    elif k > 99:\n",
    "        number = '0' + str(k)\n",
    "    plt.savefig('output/brownian-smoothing-test-%s.png' % number)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
